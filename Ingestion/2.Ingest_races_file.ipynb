{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e262203f-4b00-4372-8aa7-f84833a9ce61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client_id = dbutils.secrets.get(scope=\"formula1-scope\", key=\"client-id\")\n",
    "tenant_id = dbutils.secrets.get(scope=\"formula1-scope\", key=\"tenant-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"formula1-scope\", key=\"client-secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9964eb95-c373-4129-8553-89650c619c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.1formula1dl11.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.1formula1dl11.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.1formula1dl11.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.1formula1dl11.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.1formula1dl11.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e1e193-489b-42eb-9be8-d8a7f6ab2a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"abfss://raw@1formula1dl11.dfs.core.windows.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0955150e-5f18-49dc-9814-1ee2bf04505e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "races_df = spark.read.option(\"header\", \"true\").csv(\"abfss://raw@1formula1dl11.dfs.core.windows.net/races.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5b9a69-18be-450f-9072-1cef87a7f82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "races_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa6855c-e28d-457c-9d84-6784dfc38c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, TimestampType\n",
    "races_schema = StructType(fields=[StructField(\"raceId\", IntegerType(), False),\n",
    "                                      StructField(\"year\", IntegerType(), True),\n",
    "                                      StructField(\"round\", IntegerType(), True),\n",
    "                                      StructField(\"circuitId\", StringType(), True),\n",
    "                                      StructField(\"name\", StringType(), True),\n",
    "                                      StructField(\"date\", StringType(), True),\n",
    "                                      StructField(\"time\", StringType(), True),\n",
    "                                      StructField(\"url\", StringType(), True)])\n",
    "races_df = spark.read \\\n",
    ".schema(races_schema) \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".csv(\"abfss://raw@1formula1dl11.dfs.core.windows.net/races.csv\")\n",
    "\n",
    "races_df.show(10)\n",
    "races_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c70d65-c40a-476a-9388-85820bfa9a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, to_timestamp, concat, lit, concat_ws\n",
    "column_select_df = races_df.select(col(\"raceId\"), col(\"year\"), col(\"round\"), col(\"circuitId\"), col(\"name\"), col(\"date\"), col(\"time\"))\n",
    "\n",
    "column_rename_df = column_select_df.withColumnRenamed(\"circuitId\", \"circuit_id\") \\\n",
    ".withColumnRenamed(\"circuitRef\", \"circuit_ref\") \\\n",
    ".withColumnRenamed(\"lat\", \"latitude\") \\\n",
    ".withColumnRenamed(\"lng\", \"longitude\") \\\n",
    ".withColumnRenamed(\"alt\", \"altitude\") \\\n",
    ".withColumn(\"race_timestamp\",to_timestamp(concat_ws(\" \",col(\"date\"),col(\"time\")), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "column_rename_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304363aa-19b7-49e8-a1ec-9c169c374c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_rename_df.write.mode(\"overwrite\").parquet(\"abfss://processed@1formula1dl11.dfs.core.windows.net/races\")\n",
    "display(dbutils.fs.ls(\"abfss://processed@1formula1dl11.dfs.core.windows.net/races\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7af43f21-14dc-4140-9071-087e67f1ced3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.Ingest_races_file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
